<!DOCTYPE html>

<!--
  Google HTML5 slide template

  Authors: Luke MahÃ© (code)
           Marcin Wichary (code and design)
           
           Dominic Mazzoni (browser compatibility)
           Charles Chen (ChromeVox support)

  URL: http://code.google.com/p/html5slides/
-->


<!--

  These slides were created for the H@B workshop on 12/1.
  
  They brifly cover probability, bayes nets, naive bayes, and 
  information gain. 

  "Building" is OFF by default for easy viewing. Run "present()" in 
  a js console before presentations to turn it on.

  Yes, for those of you who disapprove I've used <center> tags 
  very liberally. I was lazy to write more CSS

-->


<html>
  <head>
    <title>H@B Artificial Intelligence Workshop</title>

    <meta charset='utf-8'>
    <script
      src='http://html5slides.googlecode.com/svn/trunk/slides.js'></script>

  </head>
  
  <style>
    .eqn {
      display: block;
      margin: auto;
      margin-top: 25px;
      margin-bottom: 25px;
    }
    .eqn-left {
      display: block;
      margin-top: 25px;
      margin-bottom: 25px;
    }
  </style>

  <body style='display: none'>

    <section class='slides layout-regular'>
      
      <!-- Your slides (<article>s) go here. Delete or comment out the
           slides below. -->
        
      <article>
        <h1>
          Sentiment Analysis using 
          <br>
          a Naive Bayes Classifier
        </h1>
        <br>
        <p>
          Allen Chen, Akihiro Matsukawa
          <br>
          Hackers@Berkeley
        </p>
      </article>
      
      <article>
        <h3>
          Today...
        </h3>
        <ul>
          <li>
            What is probabiliby, anyway?
          </li><br>
          <li>
            What are classifiers?
          </li><br>
          <li>
            So how do I build one? 
          </li><br>
          <li>
            Can you give me an example?
          </li><br>
        </ul>
      </article>

      <article>
        <h2>
          Probability
        </h2>
      </article>

      <article>
        <h3>
          Probability - Basics
        </h3>
        <p>
          We use probability to describe the <b>likelihood</b> of an event
        </p>
        <ul>
          <li>
            A <b>sample space</b> of possible things&nbsp
            <img src="http://latex.codecogs.com/png.latex?\huge&space;\inline&space;{\color{Gray}&space;\Omega&space;}">
          </li>
          <li>
            Individual outcomes from the sample space is&nbsp
            <img src="http://latex.codecogs.com/png.latex?\huge&space;\inline&space;{\color{Gray}&space;\omega&space;}">
          </li>
          <li>
            Probability is a function that assigns likelihood to each 
            &nbsp<img src="http://latex.codecogs.com/png.latex?\huge&space;\inline&space;{\color{Gray}&space;\omega&space;}">
          </li>
          <li>
            A <b>probability function</b> must satisfy 
            <img class="eqn" src="http://latex.codecogs.com/png.latex?\LARGE {\color{Gray} f\left(\omega\right)\in\left[0,1\right]\ \forall\omega\in\Omega}" title="\huge {\color{Gray} f\left(\omega\right)\in\left[0,1\right]\ \forall\omega\in\Omega}" /> 
            <img class="eqn" src="http://latex.codecogs.com/png.latex?\LARGE {\color{Gray} \sum_{\omega\in\Omega}f\left(\omega\right)=1}" title="\LARGE {\color{Gray} \sum_{\omega\in\Omega}f\left(\omega\right)=1}" />
          </li>
          <li>
            An <b>event</b> is a set of outcomes. 
            <img class="eqn" src="http://latex.codecogs.com/png.latex?\LARGE {\color{Gray} P\left(A\right)=\sum_{\omega\in A}P\left(w\right)}" title="\LARGE {\color{Gray} P\left(A\right)=\sum_{\omega\in A}P\left(w\right)}" />
          </li>
        </ul>
      </article>

      <article>
        <h3>
          Probability - Condition, Independence
        </h3>
        <p>
          <b>Conditional probability</b>: probability of event A, given B happened 
          <img class="eqn" src="http://latex.codecogs.com/png.latex?\LARGE {\color{Gray} P\left(A|B\right)=\frac{P\left(A,B\right)}{P\left(B\right)}=\frac{P\left(B|A\right)P\left(A\right)}{P\left(B\right)} }" title="\LARGE {\color{Gray} P\left(A|B\right)=\frac{P\left(A,B\right)}{P\left(B\right)}=\frac{P\left(B|A\right)P\left(A\right)}{P\left(B\right)} }" />
          <center>
          <img src="cond_1.png"> &nbsp &nbsp
          <img src="cond_2.png">
          </center>
        </p>
        <p>
          <b>Independence</b>: value of event B does not affect event A 
          <img class="eqn" src="http://latex.codecogs.com/png.latex?\LARGE {\color{Gray} P\left(A|B\right)=P\left(A\right) }" title="\LARGE {\color{Gray} P\left(A|B\right)=P\left(A\right) }" />
          <img class="eqn" src="http://latex.codecogs.com/png.latex?\LARGE {\color{Gray}P\left(A,B\right)=P\left(A\right)P\left(B\right)}" title="\LARGE {\color{Gray}P\left(A,B\right)=P\left(A\right)P\left(B\right)}" />
        </p>
      </article>

      <article>
        <h2>
          Bayesian Networks, Naive Bayes
        </h2>
      </article>

      <article>
        <h3>
          Classifier
        </h3>
        <ul>
          <li>
            Define categories, or <b>classes</b> (eg. "positive", "negative")
          </li>
          <li>
            Class is not observable, infer from what we <em>can</em> observe 
          </li>
          <li>
            Pick observations that give us insight, use as <b>features</b>
          </li>
        </ul>
        <p>
          A <b>classifier</b> takes in features, guesses a class <br><br>
          <center>
          <img src="classifier.png" width=520>
          </center>
        </p>
        <p>
          We will build a <b>Naive Bayes</b> classifier.
        </p>
      </article>

      <article>
        <h3>
          Bayesian Networks
        </h3>
        <ul>
          <li>
            Graphical representation of a <b>probability model</b>
          </li>
          <li>
            Nodes represent variables, arrows represent causality
          </li>
          <li>
            Each node has a <b>conditional probability distribution</b> conditioned on its parents. 
          </li>
        </ul>
        <p>
          <center>
          <img width=450 src="http://upload.wikimedia.org/wikipedia/en/thumb/0/0e/SimpleBayesNet.svg/500px-SimpleBayesNet.svg.png" />
          </center>
          <img class="eqn" src="http://latex.codecogs.com/png.latex?\LARGE {\color{Gray} P\left(X\right)=\prod_{x\in X}P\left(x_{i}|pa\left(X_{i}\right)\right)}" title="\LARGE {\color{Gray} P\left(X\right)=\prod_{x\in X}P\left(x_{i}|pa\left(X_{i}\right)\right)}" />
        </p>
      </article>

      <article>
        <h3>
          Naive Bayes Classifier - Definition
        </h3>
        <ul>
          <li>
            Classifier using Bayesian Networks
          </li>
          <li>
            One root node, the class 
          </li>
          <li>
            Root node points to many child nodes, the features
          </li>
          <li>
            "Naive": all features are independent of each other
          </li>
        </ul>
        <p>
          <br>
          <center>
          <img src="NB.png" width=600/>
          </center>
        </p>
      </article>

      <article>
        <h3>
          Naive Bayes Classifier - Inference
        </h3>
        <p>
          Ok, I have a vector of features. <b>How do I pick the class?</b>
        </p>
        <p>
          Answer: Pick the class c with the highest
        </p>
        <img class="eqn" src="http://latex.codecogs.com/png.latex?\LARGE {\color{Gray} P\left(C=c|F_{1},\ldots,F_{n}\right)}" title="\LARGE {\color{Gray} P\left(C=c|F_{1},\ldots,F_{n}\right)}" />
        <p>
          How do I find this conditonal probability?
          <img class="eqn-left" src="http://latex.codecogs.com/png.latex?\LARGE {\color{Gray} P\left(C|F_{1},\ldots,F_{n}\right)=\frac{P\left(C,F_{1},\ldots,F_{N}\right)}{P\left(F_{1},\ldots,F_{N}\right)}}" title="\LARGE {\color{Gray} P\left(C|F_{1},\ldots,F_{n}\right)=\frac{P\left(C,F_{1},\ldots,F_{N}\right)}{P\left(F_{1},\ldots,F_{N}\right)}}" />
          <img class="eqn-left" src="http://latex.codecogs.com/png.latex?\LARGE {\color{Gray} P\left(C|F_{1},\ldots,F_{n}\right)=\frac{P\left(C,F_{1},\ldots,F_{N}\right)}{\sum_{C}P\left(C,F_{1},\ldots,F_{N}\right)}}" title="\LARGE {\color{Gray} P\left(C|F_{1},\ldots,F_{n}\right)=\frac{P\left(C,F_{1},\ldots,F_{N}\right)}{\sum_{C}P\left(C,F_{1},\ldots,F_{N}\right)}}" />
          <img class="eqn-left" src="http://latex.codecogs.com/png.latex?\LARGE {\color{Gray} P\left(C|F_{1},\ldots,F_{n}\right)=\alpha P\left(C,F_{1},\ldots,F_{N}\right)}" title="\LARGE {\color{Gray} P\left(C|F_{1},\ldots,F_{n}\right)=\alpha P\left(C,F_{1},\ldots,F_{N}\right)}" />
          <img class=eqn-left" src="http://latex.codecogs.com/png.latex?\LARGE {\color{Gray} P\left(C|F_{1},\ldots,F_{n}\right)=\alpha P\left(C\right)\prod_{F}P\left(F_{i}|C\right)}" title="\LARGE {\color{Gray} P\left(C|F_{1},\ldots,F_{n}\right)=\alpha P\left(C\right)\prod_{F}P\left(F_{i}|C\right)}" />
        </p>
      </article>
      
      <article>
        <h3>
          Our Classifier
        </h3>
        <p>
          We will build a classifier with two classes, and "bag-of-words" style <b>binary features</b>. 
        </p>
        <p>
          <center>
          <img src="sentiment_ex.png" />
          </center>
        </p>
        <p>
          "The concert was great."
          <img class="eqn" src="http://latex.codecogs.com/png.latex?\LARGE {\color{Gray} P\left(C=+|G=1,H=0,T=1\right)=0.5*0.8*0.8*0.5=0.16 }" title="\LARGE {\color{Gray} P\left(C=+|G=1,H=0,T=1\right)=0.5*0.8*0.8*0.5=0.16 }" />
          <img class="eqn" src="http://latex.codecogs.com/png.latex?\LARGE {\color{Gray} P\left(C=-|G=1,H=0,T=1\right)=0.5*0.3*0.1*0.5=0.0075 }" title="\LARGE {\color{Gray} P\left(C=-|G=1,H=0,T=1\right)=0.5*0.3*0.1*0.5=0.0075 }" />
        </p>
        <div style="position:absolute; bottom:180px; right: 100px; padding: 5px; border:solid gray 1px">
          POSITIVE
        <div>

      </article>

      <article>
        <h3>
          Our Classifier
        </h3>
        <p>
          We will be building a Naive Bayes classifier with two classes and <b>binary features</b>.
        </p>
        <p>
          <center>
          <img src="sentiment_ex.png" />
          </center>
        </p>
        <p>
          "I hate doing the dishes."
          <img class="eqn" src="http://latex.codecogs.com/png.latex?\LARGE {\color{Gray} P\left(C=+|G=0,H=1,T=1\right)=0.5*0.2*0.2*0.5=0.01 }" title="\LARGE {\color{Gray} P\left(C=+|G=0,H=1,T=1\right)=0.5*0.2*0.2*0.5=0.01 }" />
          <img class="eqn" src="http://latex.codecogs.com/png.latex?\LARGE {\color{Gray} P\left(C=-|G=0,H=1,T=1\right)=0.5*0.7*0.9*0.5=0.1575 }" title="\LARGE {\color{Gray} P\left(C=-|G=0,H=1,T=1\right)=0.5*0.7*0.9*0.5=0.1575 }" />
        </p>
        <div style="position:absolute; bottom:180px; right: 100px; padding: 5px; border:solid gray 1px">
          NEGATIVE
        <div>

      </article>

      <article>
        <h3>
          Our Classifier
        </h3>
        <p>
          We will be building a Naive Bayes classifier with two classes and <b>binary features</b>.
        </p>
        <p>
          <center>
          <img src="sentiment_ex.png" />
          </center>
        </p>
        <p>
          "I ate cookies."
          <img class="eqn" src="http://latex.codecogs.com/png.latex?\LARGE {\color{Gray} P\left(C=+|G=0,H=0,T=0\right)=0.5*0.2*0.8*0.5=0.04}" title="\LARGE {\color{Gray} P\left(C=+|G=0,H=0,T=0\right)=0.5*0.2*0.8*0.5=0.04}" />
          <img class="eqn" src="http://latex.codecogs.com/png.latex?\LARGE {\color{Gray} P\left(C=-|G=0,H=0,T=0\right)=0.5*0.7*0.1*0.5=0.0175 }" title="\LARGE {\color{Gray} P\left(C=-|G=0,H=0,T=0\right)=0.5*0.7*0.1*0.5=0.0175 }" />
        </p>
        <div style="position:absolute; bottom:150px; right: 100px; padding: 5px; border:solid gray 1px">
          DON'T KNOW (not confident)
        <div>
      </article>

      <article>
        <h3>
          A few more things...
        </h3>
        <ul>
          <li>
            When we have a lot of features, multiplying probabilities creates underflow problems.
          </li>
          <li>
            Use sum of log probabilities. (OK, since log is monotonic)
            <img class="eqn" src="http://latex.codecogs.com/png.latex?\LARGE {\color{Gray} P\left(C\right)\prod_{F}P\left(F|C\right)\rightarrow\log{P\left(C\right)}+\sum_F{\log{P\left(F_i|C\right)}}}" title="\LARGE {\color{Gray} P\left(C\right)\prod_{F}P\left(F|C\right)\rightarrow\log{P\left(C\right)}+\sum_F{\log{P\left(F_i|C\right)}}}" />
          </li>
        </ul>
        <p>
          <h3> Let's build it! DEMO. </h3>
        </p>
      </article>

      <article>
        <h3>
          But how do we pick features?
        </h3>
        <p> 
          As we saw in the example, "the" was not a very useful feature.
          <b>But how do we know which one is useful?</b>
        </p>
        <p>
          Answer: we will use <b>entropy</b> and <b>information gain</b>
        </p>
        <p>
          <b>Entropy</b> is a measure of unpredictability. Given a set S, the entropy is 
          <img class="eqn" src="http://latex.codecogs.com/png.latex?\LARGE {\color{Gray} H\left(S\right)=\sum_{i}p_{i}\log\left(p_{i}\right) }" title="\LARGE {\color{Gray} H\left(S\right)=\sum_{i}p_{i}\log\left(p_{i}\right) }" />
        </p>
        <p>
          <b>Information gain</b> tells us the <em>improvement</em> in the entropy of a set S by splitting it into k small subsets. 
          <img class="eqn" src="http://latex.codecogs.com/png.latex?\LARGE {\color{Gray} Gain\left(S,S_{1},\ldots,S_{k}\right)=H\left(S\right)-\sum_{k}\frac{\left|S_{k}\right|}{\left|S\right|}H\left(S_{k}\right) }" title="\LARGE {\color{Gray} Gain\left(S,S_{1},\ldots,S_{k}\right)=H\left(S\right)-\sum_{k}\frac{\left|S_{k}\right|}{\left|S\right|}H\left(S_{k}\right) }" />
          eg. split into "positive" and "negative" categories
        </p>
      </article>

      <article>
        <h3>
          But how do we pick features?
        </h3>
        <p>
          Information gain gives a measure of how "informative" a feature is.
        </p>
        <p>
          So: pick features with the <b>highest</b> information gain.
        </p>
        <p>
          The entropy of a boolean distribution (a Bernoulli) is
          <img class="eqn" src="http://latex.codecogs.com/png.latex?\LARGE {\color{Gray} H\left(p\right)=p\log p+\left(1-p\right)\log\left(1-p\right) }" title="\LARGE {\color{Gray} H\left(p\right)=p\log p+\left(1-p\right)\log\left(1-p\right) }" />
          Information gain of a particular word is 
          <img class="eqn" src="http://latex.codecogs.com/png.latex?{\color{Gray} H\left(\frac{tweets\ word=T}{tweets}\right)-\frac{+tweets}{tweets}H\left(\frac{+tweets\ word=T}{+tweets}\right)-\frac{-tweets}{tweets}H\left(\frac{-tweets\ word=T}{-tweets}\right) }" title="{\color{Gray} H\left(\frac{tweets\ word=T}{tweets}\right)-\frac{+tweets}{tweets}H\left(\frac{+tweets\ word=T}{+tweets}\right)-\frac{-tweets}{tweets}H\left(\frac{-tweets\ word=T}{-tweets}\right) }" />
          <br>
          <em>
            If I split my tweets into two sets (+ve/-ve), which words am I most likely to see in one
            but not in the other?
          </em>
          </ul>
        </p>
      </article>

      <article>
        <h3> Where can I learn more? </h3>
        <ul>
          <li>
            Wikipedia:
              <a href=""></a>
              <a href=""></a>
              <a href=""></a>
              <a href=""></a>
              <a href=""></a>
          </li>
        </ul>
      </article>

    </section>

  </body>
</html>
