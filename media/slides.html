<!DOCTYPE html>

<!--
  Google HTML5 slide template

  Authors: Luke MahÃ© (code)
           Marcin Wichary (code and design)
           
           Dominic Mazzoni (browser compatibility)
           Charles Chen (ChromeVox support)

  URL: http://code.google.com/p/html5slides/
-->


<!--

  These slides were created for the H@B workshop on 12/1.
  
  They brifly cover probability, bayes nets, naive bayes, and 
  information gain. 

  "Building" is OFF by default for easy viewing. Run "present()" in 
  a js console before presentations to turn it on.

  Yes, for those of you who disapprove I've used <center> tags 
  very liberally. I was lazy to write more CSS

-->


<html>
  <head>
    <title>H@B Artificial Intelligence Workshop</title>

    <meta charset='utf-8'>
    <script
      src='http://html5slides.googlecode.com/svn/trunk/slides.js'></script>

  </head>
  
  <link rel="stylesheet" href="index.css">

  <body style='display: none'>

    <section class='slides layout-regular'>
      
      <!-- Your slides (<article>s) go here. Delete or comment out the
           slides below. -->
        
      <article>
        <h1>
          Sentiment Analysis using 
          <br>
          a Naive Bayes Classifier
        </h1>
        <br>
        <p>
          
          <br>
          Hackers@Berkeley
        </p>
        <p>
          Slides originally by Allen Chen, Akihiro Matsukawa
        </p>
	<p>
	  Revised by Peter Gao, Achal Dave
	</p>
      </article>
      
      <article>
        <h3>
          Today...
        </h3>
        <ul>
          <li>
            What is probability, anyway?
          </li><br>
          <li>
            How do Bayesian networks work?
          </li><br>
          <li>
            What are some examples of Bayesian Networks? <br>
              <ul>
                <li> Naive Bayes Classifier</li>
                <li> Markov chains </li>
              </ul>
          </li><br>
          <li>
            Can you give me an example?
          </li>
        </ul>
      </article>

      <article>
        <h2>
          Probability
        </h2>
      </article>

      <article>
        <h3>
          Probability - Basics
        </h3>
        <p>
          We use probability to describe the <b>likelihood</b> of an event
        </p>

        <h4>Details</h4>
        <ul>
          <li> 
            The <span class="definition">sample space</span> is a fancy term
            for all the possible things (<span
            class="definition">outcomes</span>) that could happen. 
          </li>
          <li>
          A <span class="definition">probability function</span> tells you what
          the probability of a particular outcome happening are.
          </li>
        </ul>
      </article>

      <article>
        <h3>
          Probability - Condition, Independence
        </h3>
        <p>
          By example:
        </p>
        <p>
          Event A: Ms. Pacman gets an A in 61B
        </p>
        <p>
          Event B: Professor Hilfinger is teaching 61B
        </p>
        <p>
          <span class="definition">Conditional probability</span>: probability
          of event A, given B happened <img class="eqn"
          src="http://latex.codecogs.com/png.latex?\LARGE {\color{Gray}
          P\left(A|B\right)=\frac{P\left(A\cap B\right)}{P\left(B\right)}}"
          title="\LARGE {\color{Gray}
          P\left(A|B\right)=\frac{P\left(A,B\right)}{P\left(B\right)}=\frac{P\left(B|A\right)P\left(A\right)}{P\left(B\right)}
          }" />
        </p>
        <p>
          <span class="definition">Independence</span>: When B doesn't
          influence the probability of A (and vice versa), A and B are
          independent.
          <img class="eqn" src="http://latex.codecogs.com/png.latex?\LARGE {\color{Gray} P\left(A|B\right)=P\left(A\right) }" title="\LARGE {\color{Gray} P\left(A|B\right)=P\left(A\right) }" />
        </p>
      </article>

      <article>
        <h2>
          Bayesian Networks, Naive Bayes
        </h2>
      </article>

      <article>
        <h3>
          Classifying
        </h3>
        <p>
          Why?
        </p>
        <ul>
          <li>Email!</li>
          <li>Winning at Pacman</li>
          <li><del>Building Skynet</del></li>
        </ul>
      </article>
      <article>
        <h3>
          Classifier
        </h3>
        <ul>
          <li>
            Define categories, or <span class="definition">classes</span> (eg. "spam", "important", "piazza spam")
          </li>
          <li>
            Pick observations, or <span class="definition">features</span>, that give us insight for the class.
          </li>
        </ul>
        <p>
          A <b>classifier</b> takes in features, guesses a class <br><br>
          <center>
          <img src="classifier.png" width=520>
          </center>
        </p>
      </article>

      <article>
        <h3>
          Bayesian Networks
        </h3>
        <ul>
          <li>
            Graphical representation of a <b>probability model</b>
          </li>
          <li>
            Nodes represent variables, arrows represent causality
          </li>
          <li>
            Each node has a <b>conditional probability distribution</b> conditioned on its parents. 
          </li>
        </ul>
        <p>
          <center>
          <img width=450 src="http://upload.wikimedia.org/wikipedia/en/thumb/0/0e/SimpleBayesNet.svg/500px-SimpleBayesNet.svg.png" />
          </center>
          <img class="eqn" src="http://latex.codecogs.com/png.latex?\LARGE {\color{Gray} P\left(X\right)=\prod_{x\in X}P\left(x_{i}|pa\left(X_{i}\right)\right)}" title="\LARGE {\color{Gray} P\left(X\right)=\prod_{x\in X}P\left(x_{i}|pa\left(X_{i}\right)\right)}" />
        </p>
      </article>

      <article>
        <h3>
          Naive Bayes Classifier - Definition
        </h3>
        <ul>
          <li>
            Classifier using Bayesian Networks
          </li>
          <li>
            One root node, the class 
          </li>
          <li>
            Root node points to many child nodes, the features
          </li>
          <li>
            "Naive": all features are independent of each other
          </li>
        </ul>
        <p>
          <br>
          <center>
          <img src="NB.png" width=600/>
          </center>
        </p>
      </article>

      <article>
        <h3>
          Naive Bayes Classifier - Inference
        </h3>
        <p>
          Ok, I have a vector of features. <b>How do I pick the class?</b>
        </p>
        <p>
          Answer: Pick the class c with the highest
        </p>
        <img class="eqn" src="http://latex.codecogs.com/png.latex?\LARGE {\color{Gray} P\left(C=c|F_{1},\ldots,F_{n}\right)}" title="\LARGE {\color{Gray} P\left(C=c|F_{1},\ldots,F_{n}\right)}" />
        <p>
          How do I find this conditonal probability?
          <img class="eqn-left" src="http://latex.codecogs.com/png.latex?\LARGE {\color{Gray} P\left(C|F_{1},\ldots,F_{n}\right)=\frac{P\left(C,F_{1},\ldots,F_{N}\right)}{P\left(F_{1},\ldots,F_{N}\right)}}" title="\LARGE {\color{Gray} P\left(C|F_{1},\ldots,F_{n}\right)=\frac{P\left(C,F_{1},\ldots,F_{N}\right)}{P\left(F_{1},\ldots,F_{N}\right)}}" />
          <img class="eqn-left" src="http://latex.codecogs.com/png.latex?\LARGE {\color{Gray} P\left(C|F_{1},\ldots,F_{n}\right)=\frac{P\left(C,F_{1},\ldots,F_{N}\right)}{\sum_{C}P\left(C,F_{1},\ldots,F_{N}\right)}}" title="\LARGE {\color{Gray} P\left(C|F_{1},\ldots,F_{n}\right)=\frac{P\left(C,F_{1},\ldots,F_{N}\right)}{\sum_{C}P\left(C,F_{1},\ldots,F_{N}\right)}}" />
          <img class="eqn-left" src="http://latex.codecogs.com/png.latex?\LARGE {\color{Gray} P\left(C|F_{1},\ldots,F_{n}\right)=\alpha P\left(C,F_{1},\ldots,F_{N}\right)}" title="\LARGE {\color{Gray} P\left(C|F_{1},\ldots,F_{n}\right)=\alpha P\left(C,F_{1},\ldots,F_{N}\right)}" />
          <img class=eqn-left" src="http://latex.codecogs.com/png.latex?\LARGE {\color{Gray} P\left(C|F_{1},\ldots,F_{n}\right)=\alpha P\left(C\right)\prod_{F}P\left(F_{i}|C\right)}" title="\LARGE {\color{Gray} P\left(C|F_{1},\ldots,F_{n}\right)=\alpha P\left(C\right)\prod_{F}P\left(F_{i}|C\right)}" />
        </p>
      </article>
      
      <article>
        <h3>
          Our Classifier
        </h3>
        <p>
          We will build a classifier with two classes, and "bag-of-words" style <b>binary features</b>. 
        </p>
        <p>
          <center>
          <img src="sentiment_ex.png" />
          </center>
        </p>
        <p>
          "The concert was great."
          <img class="eqn" src="http://latex.codecogs.com/png.latex?\LARGE {\color{Gray} P\left(C=+|G=1,H=0,T=1\right)=0.5*0.8*0.8*0.5=0.16 }" title="\LARGE {\color{Gray} P\left(C=+|G=1,H=0,T=1\right)=0.5*0.8*0.8*0.5=0.16 }" />
          <img class="eqn" src="http://latex.codecogs.com/png.latex?\LARGE {\color{Gray} P\left(C=-|G=1,H=0,T=1\right)=0.5*0.3*0.1*0.5=0.0075 }" title="\LARGE {\color{Gray} P\left(C=-|G=1,H=0,T=1\right)=0.5*0.3*0.1*0.5=0.0075 }" />
        </p>
        <div style="position:absolute; bottom:180px; right: 100px; padding: 5px; border:solid gray 1px">
          POSITIVE
        <div>

      </article>

      <article>
        <h3>
          Our Classifier
        </h3>
        <p>
          We will be building a Naive Bayes classifier with two classes and <b>binary features</b>.
        </p>
        <p>
          <center>
          <img src="sentiment_ex.png" />
          </center>
        </p>
        <p>
          "I hate doing the dishes."
          <img class="eqn" src="http://latex.codecogs.com/png.latex?\LARGE {\color{Gray} P\left(C=+|G=0,H=1,T=1\right)=0.5*0.2*0.2*0.5=0.01 }" title="\LARGE {\color{Gray} P\left(C=+|G=0,H=1,T=1\right)=0.5*0.2*0.2*0.5=0.01 }" />
          <img class="eqn" src="http://latex.codecogs.com/png.latex?\LARGE {\color{Gray} P\left(C=-|G=0,H=1,T=1\right)=0.5*0.7*0.9*0.5=0.1575 }" title="\LARGE {\color{Gray} P\left(C=-|G=0,H=1,T=1\right)=0.5*0.7*0.9*0.5=0.1575 }" />
        </p>
        <div style="position:absolute; bottom:180px; right: 100px; padding: 5px; border:solid gray 1px">
          NEGATIVE
        <div>

      </article>

      <article>
        <h3>
          Our Classifier
        </h3>
        <p>
          We will be building a Naive Bayes classifier with two classes and <b>binary features</b>.
        </p>
        <p>
          <center>
          <img src="sentiment_ex.png" />
          </center>
        </p>
        <p>
          "I ate cookies."
          <img class="eqn" src="http://latex.codecogs.com/png.latex?\LARGE {\color{Gray} P\left(C=+|G=0,H=0,T=0\right)=0.5*0.2*0.8*0.5=0.04}" title="\LARGE {\color{Gray} P\left(C=+|G=0,H=0,T=0\right)=0.5*0.2*0.8*0.5=0.04}" />
          <img class="eqn" src="http://latex.codecogs.com/png.latex?\LARGE {\color{Gray} P\left(C=-|G=0,H=0,T=0\right)=0.5*0.7*0.1*0.5=0.0175 }" title="\LARGE {\color{Gray} P\left(C=-|G=0,H=0,T=0\right)=0.5*0.7*0.1*0.5=0.0175 }" />
        </p>
        <div style="position:absolute; bottom:150px; right: 100px; padding: 5px; border:solid gray 1px">
          DON'T KNOW (not confident)
        <div>
      </article>

      <article>
        <h3>
          A few more things...
        </h3>
        <ul>
          <li>
            When we have a lot of features, multiplying probabilities creates underflow problems.
          </li>
          <li>
            Use sum of log probabilities. (OK, since log is monotonic)
            <img class="eqn" src="http://latex.codecogs.com/png.latex?\LARGE {\color{Gray} P\left(C\right)\prod_{F}P\left(F|C\right)\rightarrow\log{P\left(C\right)}+\sum_F{\log{P\left(F_i|C\right)}}}" title="\LARGE {\color{Gray} P\left(C\right)\prod_{F}P\left(F|C\right)\rightarrow\log{P\left(C\right)}+\sum_F{\log{P\left(F_i|C\right)}}}" />
          </li>
        </ul>
        <p>
          <h3> Let's build it! DEMO. </h3>
        </p>
      </article>

      <article>
        <h3>
          But how do we pick features?
        </h3>
        <p> 
          As we saw in the example, "the" was not a very useful feature.
          <b>But how do we know which one is useful?</b>
        </p>
        <p>
          Answer: we will use <b>entropy</b> and <b>information gain</b>
        </p>
        <p>
          <b>Entropy</b> is a measure of unpredictability. Given a set S, the entropy is 
          <img class="eqn" src="http://latex.codecogs.com/png.latex?\LARGE {\color{Gray} H\left(S\right)=\sum_{i}p_{i}\log\left(p_{i}\right) }" title="\LARGE {\color{Gray} H\left(S\right)=\sum_{i}p_{i}\log\left(p_{i}\right) }" />
        </p>
        <p>
          <b>Information gain</b> tells us the <em>improvement</em> in the entropy of a set S by splitting it into k small subsets. 
          <img class="eqn" src="http://latex.codecogs.com/png.latex?\LARGE {\color{Gray} Gain\left(S,S_{1},\ldots,S_{k}\right)=H\left(S\right)-\sum_{k}\frac{\left|S_{k}\right|}{\left|S\right|}H\left(S_{k}\right) }" title="\LARGE {\color{Gray} Gain\left(S,S_{1},\ldots,S_{k}\right)=H\left(S\right)-\sum_{k}\frac{\left|S_{k}\right|}{\left|S\right|}H\left(S_{k}\right) }" />
          eg. split into "positive" and "negative" categories
        </p>
      </article>

      <article>
        <h3>
          But how do we pick features?
        </h3>
        <p>
          Information gain gives a measure of how "informative" a feature is.
        </p>
        <p>
          So: pick features with the <b>highest</b> information gain.
        </p>
        <p>
          The entropy of a boolean distribution (a Bernoulli) is
          <img class="eqn" src="http://latex.codecogs.com/png.latex?\LARGE {\color{Gray} H\left(p\right)=p\log p+\left(1-p\right)\log\left(1-p\right) }" title="\LARGE {\color{Gray} H\left(p\right)=p\log p+\left(1-p\right)\log\left(1-p\right) }" />
          Information gain of a particular word is 
          <img class="eqn" src="http://latex.codecogs.com/png.latex?{\color{Gray} H\left(\frac{tweets\ word=T}{tweets}\right)-\frac{+tweets}{tweets}H\left(\frac{+tweets\ word=T}{+tweets}\right)-\frac{-tweets}{tweets}H\left(\frac{-tweets\ word=T}{-tweets}\right) }" title="{\color{Gray} H\left(\frac{tweets\ word=T}{tweets}\right)-\frac{+tweets}{tweets}H\left(\frac{+tweets\ word=T}{+tweets}\right)-\frac{-tweets}{tweets}H\left(\frac{-tweets\ word=T}{-tweets}\right) }" />
          <br>
          <em>
            If I split my tweets into two sets (+ve/-ve), which words am I most likely to see in one
            but not in the other?
          </em>
          </ul>
        </p>
      </article>

      <article>
        <h3> Where can I learn more? </h3>
        <ul>
          <li>
              Markov chains: <a href="http://en.wikipedia.org/wiki/Markov_chain">http://en.wikipedia.org/wiki/Markov_chain</a><br>
	  </li>
	  <li>
              Machine Learning: <a href="http://en.wikipedia.org/wiki/Machine_learning">http://en.wikipedia.org/wiki/Machine_learning</a><br>
	  </li>
	  <li>
              Toy Projects: <a href="https://www.quora.com/Machine-Learning/What-are-some-good-learning-projects-to-teach-oneself-about-machine-learning">https://www.quora.com/Machine-Learning/What-are-some-good-learning-projects-to-teach-oneself-about-machine-learning</a><br>
	  </li>
	  <li>
              CS188: <a href="https://berkeley.edx.org/">https://berkeley.edx.org/</a><br>
	  </li>
	  <li>
              OpenCV: <a href="http://opencv.willowgarage.com/wiki/">http://opencv.willowgarage.com/wiki/</a><br>
          </li>
        </ul>
      </article>

    </section>

  </body>
</html>
